{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "use_knn_channelislands.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**notebook für kNN-Klassifikator für channelislands**"
      ],
      "metadata": {
        "id": "jZlDmy0bSxQ-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgL915d4uAab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HUz2IqPtfLz"
      },
      "source": [
        "!git clone https://github.com/cocodataset/cocoapi\n",
        "\n",
        "!cd /content/cocoapi/PythonAPI && make"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbTFt2YihL9B"
      },
      "source": [
        " !pip install timm\n",
        " \n",
        " !git clone https://github.com/Moldazien/BA.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyfQxEwgMg8R"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/BA')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/dataset #kopieren aus gdrive damit es schneller geht\n",
        "!cp -R /content/gdrive/MyDrive/Datasets/channelisland/features /content/dataset/features"
      ],
      "metadata": {
        "id": "UFWM2uT369E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4No_m7wkJtI2"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import cv2\n",
        "import random\n",
        "import colorsys\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "import skimage.io\n",
        "from skimage.measure import find_contours\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Polygon\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms as pth_transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import utils\n",
        "import vision_transformer as vits\n",
        "\n",
        "from pycocotools.coco import COCO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mXt9tcRQbyP"
      },
      "source": [
        "import os\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Test_Dataset(Dataset):\n",
        "  def __init__(self, annotations_file, dataset_dir):  #taxonomy must be: kingdom phylum class order family genus name\n",
        "    self.annotations_file = annotations_file\n",
        "    self.coco = COCO(annotations_file)  #for fast reading\n",
        "    self.dataset = dataset_dir\n",
        "\n",
        "    self.ann_ids = self.coco.getAnnIds()\n",
        "    self.img_ids = self.coco.getImgIds()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ann_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    ann_id = self.ann_ids[idx]\n",
        "    ann = self.coco.loadAnns([ann_id])\n",
        "    img_id = ann[0]['image_id']\n",
        "    img = self.coco.loadImgs([img_id])\n",
        "    fileN = img[0]['file_name'].split('/')\n",
        "    classN = fileN[0]\n",
        "    name = classN.split('.')[0]\n",
        "    feature_path = self.dataset + '/features/' + name + '_feature.pt'\n",
        "    features = torch.load(feature_path, map_location=torch.device('cpu'))\n",
        "    ground_truth = ann[0]['category_id']\n",
        "    return ground_truth, features, ann[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Train_Dataset(Dataset):\n",
        "  def __init__(self, annotations_file, dataset_dir):  #taxonomy must be: kingdom phylum class order family genus name\n",
        "    self.annotations_file = annotations_file\n",
        "    self.coco = COCO(annotations_file)\n",
        "    self.dataset = dataset_dir\n",
        "    self.img_ids = self.coco.getImgIds()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_id = self.img_ids[idx]\n",
        "    img = self.coco.loadImgs([img_id])\n",
        "    ann_ids = self.coco.getAnnIds([img_id])\n",
        "    anns = self.coco.loadAnns(ann_ids)\n",
        "    fileN = img[0]['file_name'].split('/')\n",
        "    classN = fileN[0]\n",
        "    direcN = fileN[1]\n",
        "    imgN = fileN[2].split('.')[0]\n",
        "    feature_path = self.dataset + '/features/' + classN + '/' + direcN + '/' + imgN + '_feature.pt'\n",
        "    features = torch.load(feature_path, map_location=torch.device('cpu'))\n",
        "    ground_truth = 0\n",
        "    if len(anns) > 0:\n",
        "      ground_truth = anns[0]['category_id']\n",
        "    return ground_truth, features"
      ],
      "metadata": {
        "id": "0XzRaKQEkmoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgmHg8XqVoPV"
      },
      "source": [
        "dataset_path = '/content/dataset'\n",
        "\n",
        "test_path = 'DATASET_PATH'\n",
        "\n",
        "trainset = Train_Dataset('TRAINANNOTATIONS.json', dataset_path)\n",
        "testset = Test_Dataset('TESTANNOTATIONS.json', test_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDno7KwGVx98"
      },
      "source": [
        "import random\n",
        "random.seed(7)\n",
        "\n",
        "train_ids = np.arange(0,trainset.__len__(),1)\n",
        "test_ids = np.arange(0,testset.__len__(),1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHazNry9W-N4"
      },
      "source": [
        "#laden von trainingsvektoren und groundtruth, um knn zu trainieren\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "train_ids = np.arange(0,trainset.__len__(),1)\n",
        "\n",
        "for i in train_ids:\n",
        "  gt, features = trainset.__getitem__(i)\n",
        "  features = features.reshape(-1)\n",
        "  npfeatures = np.asarray(features)\n",
        "  X.append(npfeatures)\n",
        "  Y.append(gt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsvqKTQK3EIs"
      },
      "source": [
        "X_arr = np.asarray(X)\n",
        "Y_arr = np.asarray(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_HSqVQ54bAT"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier as kNN\n",
        "\n",
        "clf = kNN(n_neighbors=200)  #training und neighbors\n",
        "X_svm = X_arr\n",
        "Y_svm = Y_arr\n",
        "clf.fit(X_svm, Y_svm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uWz4nMl3rTq"
      },
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "dump(clf, 'FILE_PATH.joblib')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing**"
      ],
      "metadata": {
        "id": "9gsxrxBHJN5z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqLHMWbC5fV7"
      },
      "source": [
        "from joblib import dump, load\n",
        "clf = load('FILE_PATH.joblib') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcZ1b9VgejeV"
      },
      "source": [
        "from time import time\n",
        "\n",
        "test_ids = np.arange(0,testset.__len__(),1)\n",
        "\n",
        "pred_anns = []\n",
        "\n",
        "Ygt = []\n",
        "Ypred = []\n",
        "\n",
        "counter = 1\n",
        "\n",
        "start_time = time() #zeit messen, weils interessant ist\n",
        "\n",
        "for i in test_ids:\n",
        "  gt, features, annotat = testset.__getitem__(i)\n",
        "  features = features.reshape(1, -1)\n",
        "  npfeatures = np.asarray(features)\n",
        "  Ygt.append(gt)\n",
        "  pred = clf.predict(npfeatures)\n",
        "  counter = counter + 1\n",
        "  Ypred.append(pred[0])\n",
        "  prediction = int(pred[0])\n",
        "  annotat['knn_pred'] =  prediction\n",
        "  pred_anns.append(annotat)\n",
        "\n",
        "end_time = time()\n",
        "\n",
        "elapsed = end_time - start_time\n",
        "print(elapsed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json \n",
        "\n",
        "output_file = 'RESULTS_FILE.json'\n",
        "\n",
        "#with open(output_file, 'w') as file:\n",
        "#  json.dump(pred_anns, file, indent = 4, sort_keys=False)  "
      ],
      "metadata": {
        "id": "gAOTrz5u5yGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbyUTzD6nnst"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix as confusion\n",
        "\n",
        "matr = confusion(Ygt, Ypred, labels = [1,2,3,4])  #confusionmatrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLDWg9l-r7UF"
      },
      "source": [
        "norm_matrix = np.zeros(matr.shape)\n",
        "\n",
        "for i in range(matr.shape[0]):\n",
        "  for j in range(matr.shape[1]):\n",
        "    norm_matrix[i,j] = matr[i,j]/sum(matr[i,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jUwukOsq3Ct"
      },
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# confusionmatrix ausgabe in schön\n",
        "disp = ConfusionMatrixDisplay(matr)\n",
        "disp.plot()\n",
        "\n",
        "disp1 = ConfusionMatrixDisplay(norm_matrix)\n",
        "disp1.plot()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, average_precision_score, f1_score, precision_score, recall_score\n",
        "\n",
        "accuracy = accuracy_score(Ygt, Ypred)\n",
        "f1 = f1_score(Ygt, Ypred, average = None)\n",
        "precision = precision_score(Ygt, Ypred, average = None)\n",
        "recall = recall_score(Ygt, Ypred, average = None)"
      ],
      "metadata": {
        "id": "77FZOH5iBLte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MzdObuvtKZ1"
      },
      "source": [
        "print(accuracy)\n",
        "print(f1)\n",
        "print(precision)\n",
        "print(recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA, für iNaturalist Datensatz"
      ],
      "metadata": {
        "id": "Y5XT4xou2EO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "\n",
        "x = X_arr\n",
        "\n",
        "x = StandardScaler().fit_transform(x)\n",
        "y = Y_arr\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "principalComponents = pca.fit_transform(x)\n",
        "\n",
        "principalDf = pd.DataFrame(data = principalComponents\n",
        "             , columns = ['component 1', 'component 2'])\n",
        "\n",
        "\n",
        "targetDf = pd.DataFrame(data = y, columns  = ['target'])\n",
        "\n",
        "finalDf = pd.concat([principalDf, targetDf[['target']]], axis = 1)"
      ],
      "metadata": {
        "id": "-ZPFr_GJ2Fzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "\n",
        "print(matr.shape)\n",
        "\n",
        "targets = list(np.arange(0, matr.shape[0]+1, 1))\n",
        "\n",
        "def get_cmap(n, name='hsv'):\n",
        "    return plt.cm.get_cmap(name, n)\n",
        "\n",
        "new_cmap = get_cmap(matr.shape[0]+1)\n",
        "\n",
        "\n",
        "for i in range(matr.shape[0]+1):\n",
        "    target = targets[i]\n",
        "\n",
        "    indicesToKeep = finalDf['target'] == target\n",
        "    ax.scatter(finalDf.loc[indicesToKeep, 'component 1']\n",
        "               , finalDf.loc[indicesToKeep, 'component 2']\n",
        "               , c = new_cmap(i)\n",
        "               , s = 0.5)\n",
        "#ax.legend(targets)\n",
        "ax.grid()"
      ],
      "metadata": {
        "id": "exdL8Q2u3tiy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}