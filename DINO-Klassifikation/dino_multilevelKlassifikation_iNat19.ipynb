{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dino_multilevelKlassifikation_iNat19.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**notebook für multilevelklassifikation auf iNat19 Datensatz. (Training, Testing, Metriken)**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**ex gibt 2 verschiedene trainingsmethoden. die erste kann nur eine batchsize von 1 haben, ist dafür aber auch weniger aufwendig auszuführen, und eher zum testen des mappings der klassen gedacht. die zweite methode trainiert jedes mlp manuell einzeln mit eine beliebigen batchsize**"
      ],
      "metadata": {
        "id": "5HsIGxOcnqAO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgL915d4uAab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HUz2IqPtfLz"
      },
      "source": [
        "!git clone https://github.com/cocodataset/cocoapi\n",
        "\n",
        "!cd /content/cocoapi/PythonAPI && make"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbTFt2YihL9B"
      },
      "source": [
        " !pip install timm\n",
        " \n",
        " !git clone https://github.com/Moldazien/BA.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyfQxEwgMg8R"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/BA')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o56k4XbkI4vo",
        "outputId": "22ae62b6-1e66-4899-a79f-f0188d6c0bc9"
      },
      "source": [
        "!cd BA && git pull"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 0: cd: BA: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/dataset\n",
        "!cp -R PATH_TO_FEATURES /content/dataset/features"
      ],
      "metadata": {
        "id": "UFWM2uT369E_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4No_m7wkJtI2"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import cv2\n",
        "import random\n",
        "import colorsys\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "import skimage.io\n",
        "from skimage.measure import find_contours\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Polygon\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms as pth_transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import utils\n",
        "import vision_transformer as vits\n",
        "\n",
        "from pycocotools.coco import COCO"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "P-Gg6qJqh3Py"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mXt9tcRQbyP"
      },
      "source": [
        "import os\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Seg_Dataset(Dataset):\n",
        "  def __init__(self, annotations_file, dataset_dir, taxonomy, train):  #taxonomy must be: kingdom phylum class order family genus name\n",
        "    self.train = train\n",
        "    self.annotations_file = annotations_file\n",
        "    self.coco = COCO(annotations_file)  #for fast reading\n",
        "    self.toplvl = 'genus'######################################################## #muss hier verändert werden, um auf anderer ebene zu testen\n",
        "    \n",
        "    self.dataset = dataset_dir\n",
        "    self.img_ids = self.coco.getImgIds()\n",
        "\n",
        "    def mapping(taxonomy):\n",
        "      mapping = {}\n",
        "      cat_ids = self.coco.getCatIds()\n",
        "      categories = self.coco.loadCats(cat_ids)\n",
        "      by_tax_cat = list(set([cat[taxonomy] for cat in categories]))\n",
        "      by_tax_cat.sort()\n",
        "      numb_cats = len(by_tax_cat)\n",
        "      for i in range(numb_cats):\n",
        "        for cat in categories:\n",
        "          if cat[taxonomy] == by_tax_cat[i]:\n",
        "            mapping[cat['id']] = i + 1\n",
        "      return mapping\n",
        "      \n",
        "    self.upcls_mapping = mapping(self.toplvl)\n",
        "    self.upcls_len = len(self.upcls_mapping)\n",
        "    self.lower_mapping = mapping('name')\n",
        "    self.lower_len = len(self.lower_mapping)\n",
        "\n",
        "    def reverse_mapping(taxonomy, name_mapping):\n",
        "      gen_to_name = {}\n",
        "      cat_ids = self.coco.getCatIds()\n",
        "      categories = self.coco.loadCats(cat_ids)\n",
        "      by_tax_cat = list(set([cat[taxonomy] for cat in categories]))\n",
        "      by_tax_cat.sort() \n",
        "      numb_cats = len(by_tax_cat)\n",
        "      for i in range(numb_cats):\n",
        "        gen_list = []\n",
        "        for elem in categories:\n",
        "          if elem[taxonomy] == by_tax_cat[i]:\n",
        "            gen_list.append(elem['id'])\n",
        "        gen_to_name[i+1] = gen_list\n",
        "      for elem in gen_to_name:\n",
        "        for i in range(len(gen_to_name[elem])):\n",
        "          gen_to_name[elem][i] = name_mapping[gen_to_name[elem][i]]\n",
        "      return gen_to_name\n",
        "\n",
        "    self.name_genus = reverse_mapping(self.toplvl, self.lower_mapping)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_id = self.img_ids[idx]\n",
        "    img = self.coco.loadImgs(img_id)\n",
        "    ann_ids = self.coco.getAnnIds(img[0]['id'])\n",
        "    anns = self.coco.loadAnns(ann_ids)\n",
        "    fileN = img[0]['file_name'].split('/')\n",
        "\n",
        "    classN = fileN[1]\n",
        "    direcN = fileN[2]\n",
        "    imgN = fileN[3].split('.')[0]\n",
        "    feature_path = self.dataset + '/features/' + classN + '/' + direcN + '/' + imgN + '_feature.pt'\n",
        "    features = torch.load(feature_path, map_location=torch.device('cpu'))\n",
        "    ground_truth = 0\n",
        "    gt_genus = 0\n",
        "    if len(anns) > 0:\n",
        "      ground_truth = self.lower_mapping[anns[0]['category_id']]\n",
        "      gt_genus = self.upcls_mapping[anns[0]['category_id']]\n",
        "    super_cut = self.name_genus[gt_genus]\n",
        "    gt_vect = np.zeros(len(super_cut))\n",
        "\n",
        "    for i in range(len(super_cut)):\n",
        "      if ground_truth == super_cut[i]:\n",
        "        gt_vect[i] = 1\n",
        "    gt_vect = torch.tensor(gt_vect)\n",
        "    gt = None\n",
        "\n",
        "    if self.train:\n",
        "      gt = gt_vect\n",
        "    else:\n",
        "      gt = ground_truth\n",
        "\n",
        "    return features, gt, gt_genus "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "#datenklasse, um aus obiger datenklasse nur elemente zu bekommen, die aus einer bestimmten klasse der höheren Taxonomieebene sind\n",
        "class PerKlassDataset(Dataset):\n",
        "  def __init__(self, dataset, trainingsschritt):  #taxonomy must be: kingdom phylum class order family genus name\n",
        "    self.subdataset = dataset \n",
        "    self.step = trainingsschritt    #variable die beschreibt, bei welcher höheren klasse man gerade ist\n",
        "\n",
        "    self.ann_file = self.subdataset.coco\n",
        "    self.needed_ids = self.subdataset.name_genus[self.step]\n",
        "\n",
        "    self.cut_img_ids = []\n",
        "\n",
        "    self.name_genus = self.subdataset.name_genus\n",
        "\n",
        "    for id in self.subdataset.img_ids:\n",
        "      img =self.subdataset.coco.loadImgs(id)\n",
        "      ann_ids = self.subdataset.coco.getAnnIds(img[0]['id'])\n",
        "      anns = self.subdataset.coco.loadAnns(ann_ids)\n",
        "      if len(anns) > 0:\n",
        "        cat_id = self.subdataset.lower_mapping[anns[0]['category_id']]\n",
        "        if cat_id in self.needed_ids:\n",
        "          self.cut_img_ids.append(self.subdataset.img_ids.index(id))\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.cut_img_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    real_id = self.cut_img_ids[idx]\n",
        "    features, gt, gt_genus = self.subdataset.__getitem__(real_id)\n",
        "\n",
        "    return features, gt, gt_genus "
      ],
      "metadata": {
        "id": "dEvhYmNieC0a"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgmHg8XqVoPV"
      },
      "source": [
        "dataset_path = '/content/dataset'\n",
        "\n",
        "trainset = Seg_Dataset('TRAIN_ANNOTATONS.json', dataset_path, 'name', True) #muss hier immer name sein\n",
        "testset = Seg_Dataset('TEST_ANNOTATONS.json', dataset_path, 'name', False) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(testset.name_genus)\n",
        "\n",
        "for i in testset.name_genus:\n",
        "  print(len(testset.name_genus[i]))"
      ],
      "metadata": {
        "id": "pbERKBXRryO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDno7KwGVx98"
      },
      "source": [
        "import random\n",
        "random.seed(7)\n",
        "\n",
        "train_ids = np.arange(0,trainset.__len__(),1)\n",
        "test_ids = np.arange(0,testset.__len__(),1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model\n"
      ],
      "metadata": {
        "id": "ger5houTkWD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MLP_net(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "      super(MLP_net, self).__init__()\n",
        "\n",
        "      self.lin1 = nn.Linear(input_size, input_size, bias=True)\n",
        "      self.lin2 = nn.Linear(input_size, input_size, bias=True)\n",
        "      #self.lin2plus = nn.Linear(input_size, input_size, bias=True)\n",
        "      self.lin3 = nn.Linear(input_size, int(input_size/2), bias=True)\n",
        "      self.lin3plus = nn.Linear(int(input_size/2), int(input_size/2), bias=True)\n",
        "      #self.lin3plus1 = nn.Linear(int(input_size/2), int(input_size/2), bias=True)\n",
        "      self.lin4 = nn.Linear(int(input_size/2), output_size, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.lin1(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.lin2(x)\n",
        "      x = F.relu(x)\n",
        "      #x = self.lin2plus(x)\n",
        "      #x = F.relu(x)\n",
        "      x = self.lin3(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.lin3plus(x)\n",
        "      x = F.relu(x)\n",
        "      #x = self.lin3plus1(x)\n",
        "      #x = F.relu(x)\n",
        "      x = self.lin4(x)\n",
        "      #x = F.softmax(x, dim = 0)#F.sigmoid(x)#\n",
        "      return x"
      ],
      "metadata": {
        "id": "b9w7-X1fkVYA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_models = False\n",
        "PATH = 'PATH/MODEL'\n",
        "network_list = []\n",
        "\n",
        "#erstellen einer liste von k verschiedenen modellen (14 für gattung)\n",
        "\n",
        "for i in range(len(trainset.name_genus)):\n",
        "  print(i)\n",
        "  net = net = MLP_net(768, len(trainset.name_genus[i+1]))\n",
        "  net.to(device)\n",
        "\n",
        "  if load_models:\n",
        "    full_path = PATH + str(i+1) + '.pth'\n",
        "    net.load_state_dict(torch.load(full_path))\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  network_list.append(net)\n",
        "\n",
        "\n",
        "#for i in range(len(trainset.name_genus)):\n",
        "#  print('Saved ' + str(i + 1))\n",
        "#  full_path = PATH + str(i + 1) + '.pth'"
      ],
      "metadata": {
        "id": "QoGHBwPllAeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "#erstellen einer liste von optimizern. je ein optimizer pro mlp in der liste\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_list = []\n",
        "\n",
        "for i in range(len(trainset.name_genus)):\n",
        "  print(i)\n",
        "  optimizer = optim.Adam(network_list[i].parameters(), lr = 0.001) #0.001\n",
        "  optimizer_list.append(optimizer)"
      ],
      "metadata": {
        "id": "FCAf-MNQlB4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**naives training mit batchsize von 1. nur zum testen gedacht. hier muss nicht jedes mlp einzeln manuell trainiert werden**"
      ],
      "metadata": {
        "id": "ItUI1sQxb1Ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "dataset_path = '/content/dataset'\n",
        "\n",
        "trainset = Seg_Dataset('TRAIN_ANNOTATONS.json', dataset_path, 'name', True) #muss hier name sein\n",
        "\n",
        "batch_size = 1  #geht hier nicht anders\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "K4nw8LLbk-c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(30):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, mask, gt_genus = data\n",
        "\n",
        "        #print(gt_genus)\n",
        "        gt_genus = int(gt_genus[0])\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        inputs = inputs.reshape(768)\n",
        "\n",
        "        mask = mask.to(device)\n",
        "        mask = mask.type(torch.float32)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer_list[gt_genus-1].zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "\n",
        "        outputs = network_list[gt_genus-1](inputs)\n",
        "       \n",
        "        outputs = outputs.reshape((1, len(trainset.name_genus[gt_genus]))) #keine ahnung\n",
        "\n",
        "        #print(mask)\n",
        "        #print(outputs.shape)\n",
        "        #print(trainset.name_genus[gt_genus])\n",
        "\n",
        "        loss = criterion(outputs, mask)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer_list[gt_genus-1].step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        loss_graph[gt_genus-1].append(loss.item())\n",
        "\n",
        "        output_value = 2000\n",
        "        if i % output_value == output_value - 1:    # print every 2000 mini-batches\n",
        "          print('[%d, %5d] loss: %.20f' %\n",
        "              (epoch + 1, i + 1, running_loss / output_value))\n",
        "          \n",
        "          full_loss_graph.append(running_loss / output_value)\n",
        "\n",
        "          running_loss = 0.0\n",
        "          #PATH = '/content/gdrive/MyDrive/models/nets/simple/' + str(i) + 'model_mlp.pth'\n",
        "          #torch.save(net.state_dict(), PATH)\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "PATH = 'MODELS_DIRECTORY'\n",
        "\n",
        "for i in range(len(trainset.name_genus)): #speichern aller einzelnen modelle\n",
        "  print('Saved ' + str(i + 1))\n",
        "  full_path = PATH + str(i + 1) + 'genus.pth'\n",
        "  torch.save(net.state_dict(), full_path)"
      ],
      "metadata": {
        "id": "v0BH2jSklEDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = 500\n",
        "\n",
        "#gesamtloss\n",
        "for item in loss_graph:\n",
        "  loss_arr = np.asarray(item)\n",
        "  loss_arr = np.convolve(loss_arr, np.ones(N)/N, mode='valid')\n",
        "\n",
        "  x_val = np.arange(0, loss_arr.size, 1)\n",
        "\n",
        "  plt.plot(x_val, loss_arr)\n",
        "  plt.ylabel('loss')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "6MIN2--h4mbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#loss per class in höherer taxonomieebene\n",
        "loss_arr = np.asarray(full_loss_graph)\n",
        "x_val = np.arange(0, len(full_loss_graph), 1)\n",
        "\n",
        "plt.plot(x_val, loss_arr)\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MFkATBrK1-H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**manuelles Training für jedes einzelnen mlp mit verschiedenen batchsizes**\n",
        "\n",
        "**Training muss einmal für alle MLPs einzeln durchgeführt werden (beispielsweise für die Gattung mit 1-14). Danach kann inferenz ausgeführt werden** "
      ],
      "metadata": {
        "id": "-_0DG-C8hUwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/dataset'\n",
        "\n",
        "trainset1 = Seg_Dataset('/content/gdrive/MyDrive/Datasets/iNaturalist2019/annotations/meg_train.json', dataset_path, 'name', True) #class order family genus name\n",
        "\n",
        "trainset = PerKlassDataset(trainset1, 1)  #startet bei 1. (genus bis 14)\n",
        "\n",
        "testset = Seg_Dataset('/content/gdrive/MyDrive/Datasets/iNaturalist2019/annotations/meg_test.json', dataset_path, 'name', False)\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "KjcbRICdhdh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(30):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, mask, gt_genus = data\n",
        "        gt_genus = int(gt_genus[0])\n",
        "        inputs = inputs.to(device)\n",
        "        numb_inst = inputs.shape[0]\n",
        "        #inputs = inputs.reshape(768)\n",
        "\n",
        "        mask = mask.to(device)\n",
        "        mask = mask.type(torch.float32)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer_list[gt_genus-1].zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "\n",
        "        outputs = network_list[gt_genus-1](inputs)\n",
        "       \n",
        "        outputs = outputs.reshape((numb_inst, len(trainset.name_genus[gt_genus]))) #keine ahnung\n",
        "\n",
        "        #print(mask)\n",
        "        #print(outputs.shape)\n",
        "        #print(trainset.name_genus[gt_genus])\n",
        "\n",
        "        loss = criterion(outputs, mask)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer_list[gt_genus-1].step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        #output_value = 2000\n",
        "        #if i % output_value == output_value - 1:    # print every 2000 mini-batches\n",
        "        #  print('[%d, %5d] loss: %.20f' %\n",
        "        #      (epoch + 1, i + 1, running_loss / output_value))\n",
        "        #  \n",
        "        #  full_loss_graph.append(running_loss / output_value)\n",
        "        #\n",
        "        #  running_loss = 0.0\n",
        "        #  #PATH = '/content/gdrive/MyDrive/models/nets/simple/' + str(i) + 'model_mlp.pth'\n",
        "        #  #torch.save(net.state_dict(), PATH)\n",
        "\n",
        "print('Finished Training')\n",
        "PATH = 'MODELS_DIRECTORY'\n",
        "for i in range(len(trainset.name_genus)): #speichern aller einzelnen modelle\n",
        "  print('Saved ' + str(i + 1))\n",
        "  full_path = PATH + str(i + 1) + 'genus.pth'\n",
        "  #torch.save(net.state_dict(), full_path)"
      ],
      "metadata": {
        "id": "EyH1LkP5iABM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**knn-Klassifikator wird hier geladen. falls kein modell vorhanden ist, ist ganz unten im notebook ein bereich um ein knn klassifikator zu trainiern**"
      ],
      "metadata": {
        "id": "TIRmyJmsmzLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier as kNN\n",
        "from joblib import dump, load\n",
        "\n",
        "clf = kNN(n_neighbors=20)\n",
        "\n",
        "clf = load('KNN.joblib') "
      ],
      "metadata": {
        "id": "PSlP_fZHi1q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n_Rt8FI8Nu1"
      },
      "source": [
        "**testing auf trainingsdatensatz**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcZ1b9VgejeV"
      },
      "source": [
        "#testing auf testdatensatz. errechnete ergebnisse der mlps werden auf die einzelnen klassen auf speziesebene umgerechnet\n",
        "\n",
        "Ygt = []\n",
        "Ypred = []\n",
        "\n",
        "net.eval()\n",
        "\n",
        "counter = 0\n",
        "\n",
        "for i in test_ids:\n",
        "  features, gt, gt_genus = testset.__getitem__(i)\n",
        "\n",
        "  net_features = features.reshape(768)\n",
        "  npfeatures = np.asarray(features)\n",
        "\n",
        "  knn_pred = clf.predict(npfeatures)[0] #knn prediction\n",
        "  species = testset.name_genus[knn_pred]\n",
        "\n",
        "  Ygt.append(gt)\n",
        "\n",
        "  net_features = net_features.to(device)\n",
        "  net_pred = network_list[knn_pred-1](net_features)\n",
        "  net_pred = net_pred.detach()\n",
        "  net_pred = net_pred.to('cpu')\n",
        "\n",
        "  net_pred = F.softmax(net_pred, dim = 0)\n",
        "\n",
        "  pred_array = np.asarray(net_pred, dtype=np.float32)\n",
        "  \n",
        "  top = np.amax(pred_array)\n",
        "  class_pred = int(np.where(pred_array == top)[0][0])\n",
        "  class_pred = species[class_pred]\n",
        "  \n",
        "\n",
        "  Ypred.append(class_pred)\n",
        "  counter = counter + 1"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "YclassesGt = Ygt\n",
        "YclassesPred = Ypred"
      ],
      "metadata": {
        "id": "MDhw5t_EXwE5"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ypred = YclassesPred\n",
        "Ygt = YclassesGt"
      ],
      "metadata": {
        "id": "Z62xeAiwG7_s"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbyUTzD6nnst"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix as confusion\n",
        "\n",
        "matr = confusion(Ygt, Ypred)  #confusionmatrix"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLDWg9l-r7UF"
      },
      "source": [
        "norm_matrix = np.zeros(matr.shape)\n",
        "\n",
        "for i in range(matr.shape[0]):\n",
        "  for j in range(matr.shape[1]):\n",
        "    norm_matrix[i,j] = matr[i,j]/sum(matr[i,:])\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jUwukOsq3Ct"
      },
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "#confusionmatrizen in schön\n",
        "disp = ConfusionMatrixDisplay(matr)\n",
        "disp.plot()\n",
        "\n",
        "disp1 = ConfusionMatrixDisplay(norm_matrix)\n",
        "disp1.plot()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, average_precision_score, f1_score, precision_score, recall_score\n",
        "\n",
        "accuracy = accuracy_score(Ygt, Ypred)\n",
        "#avg_prec = average_precision_score(Ygt, Ypred)\n",
        "f1 = f1_score(Ygt, Ypred, average = 'macro')\n",
        "precision = precision_score(Ygt, Ypred, average = 'macro')\n",
        "recall = recall_score(Ygt, Ypred, average = 'macro')"
      ],
      "metadata": {
        "id": "77FZOH5iBLte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MzdObuvtKZ1"
      },
      "source": [
        "print(accuracy)\n",
        "print(f1)\n",
        "print(precision)\n",
        "print(recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**training für kNN-Klassifikator für oben**"
      ],
      "metadata": {
        "id": "_GUyUx3VZeTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = Seg_Dataset('TRAIN_ANNOTATONS.json', dataset_path, 'genus', False)"
      ],
      "metadata": {
        "id": "WNqIDUIzZhcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(7)\n",
        "\n",
        "train_ids = np.arange(0,trainset.__len__(),1)"
      ],
      "metadata": {
        "id": "lYsu_rwbZp_t"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "Y = []\n",
        "\n",
        "\n",
        "for i in train_ids:\n",
        "  features, speciec_gt, gt = trainset.__getitem__(i)\n",
        "  \n",
        "  print(i)\n",
        "\n",
        "  features = features.reshape(-1)\n",
        "\n",
        "  npfeatures = np.asarray(features)\n",
        "  \n",
        "  X.append(npfeatures)\n",
        "  Y.append(gt)"
      ],
      "metadata": {
        "id": "uR42ziRhZwDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X))\n",
        "print(len(Y))"
      ],
      "metadata": {
        "id": "derZhbOAaCkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_arr = np.asarray(X)\n",
        "Y_arr = np.asarray(Y)\n",
        "\n",
        "print(X_arr.shape)\n",
        "print(Y_arr.shape)"
      ],
      "metadata": {
        "id": "7YkzhMHbaDvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier as kNN\n",
        "\n",
        "clf = kNN(n_neighbors=20)\n",
        "\n",
        "X_svm = X_arr\n",
        "Y_svm = Y_arr\n",
        "\n",
        "clf.fit(X_svm, Y_svm)"
      ],
      "metadata": {
        "id": "PSF4OGufaFNs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}