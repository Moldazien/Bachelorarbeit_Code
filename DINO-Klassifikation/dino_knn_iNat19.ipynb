{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dino_knn_iNat19.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**notebook um kNN-Klassifikator auf iNat19 zu verwenden (training, test, eval)**"
      ],
      "metadata": {
        "id": "yOC-GUsCUh9e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgL915d4uAab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HUz2IqPtfLz"
      },
      "source": [
        "!git clone https://github.com/cocodataset/cocoapi\n",
        "\n",
        "!cd /content/cocoapi/PythonAPI && make"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbTFt2YihL9B"
      },
      "source": [
        " !pip install timm\n",
        " \n",
        " !git clone https://github.com/Moldazien/BA.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyfQxEwgMg8R"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/BA')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/dataset   #kopieren aus google drive, um schneller zu machen\n",
        "!cd dataset && mkdir features\n",
        "!cp -R FEATURE_PATH /content/dataset/features"
      ],
      "metadata": {
        "id": "UFWM2uT369E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4No_m7wkJtI2"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import cv2\n",
        "import random\n",
        "import colorsys\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "import skimage.io\n",
        "from skimage.measure import find_contours\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Polygon\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms as pth_transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import utils\n",
        "import vision_transformer as vits\n",
        "\n",
        "from pycocotools.coco import COCO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mXt9tcRQbyP"
      },
      "source": [
        "import os\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Seg_Dataset(Dataset):\n",
        "  def __init__(self, annotations_file, dataset_dir, taxonomy):  #taxonomy must be: kingdom phylum class order family genus name\n",
        "    self.annotations_file = annotations_file\n",
        "    self.coco = COCO(annotations_file) \n",
        "    self.dataset = dataset_dir\n",
        "    self.img_ids = self.coco.getImgIds()\n",
        "\n",
        "    def mapping(taxonomy):\n",
        "      mapping = {}\n",
        "      cat_ids = self.coco.getCatIds()\n",
        "      categories = self.coco.loadCats(cat_ids)\n",
        "      by_tax_cat = list(set([cat[taxonomy] for cat in categories]))\n",
        "      by_tax_cat.sort()\n",
        "      numb_cats = len(by_tax_cat)\n",
        "      for i in range(numb_cats):\n",
        "        for cat in categories:\n",
        "          if cat[taxonomy] == by_tax_cat[i]:\n",
        "            mapping[cat['id']] = i + 1\n",
        "      return mapping\n",
        "      \n",
        "    self.cat_mapping = mapping(taxonomy)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_id = self.img_ids[idx]\n",
        "    img = self.coco.loadImgs(img_id)\n",
        "    ann_ids = self.coco.getAnnIds(img[0]['id'])\n",
        "    anns = self.coco.loadAnns(ann_ids)\n",
        "    fileN = img[0]['file_name'].split('/')\n",
        "    classN = fileN[1]\n",
        "    direcN = fileN[2]\n",
        "    imgN = fileN[3].split('.')[0]\n",
        "    feature_path = self.dataset + '/features/' + classN + '/' + direcN + '/' + imgN + '_feature.pt'\n",
        "    features = torch.load(feature_path, map_location=torch.device('cpu'))\n",
        "    ground_truth = 0\n",
        "    if len(anns) > 0:\n",
        "      ground_truth = self.cat_mapping[anns[0]['category_id']]\n",
        "    return ground_truth, features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import os\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Test_Dataset(Dataset):\n",
        "  def __init__(self, annotations_file, dataset_dir):  #taxonomy must be: kingdom phylum class order family genus name\n",
        "    self.annotations_file = annotations_file\n",
        "    self.coco = COCO(annotations_file)  #for fast reading\n",
        "    \n",
        "    self.dataset = dataset_dir\n",
        "\n",
        "    self.ann_ids = self.coco.getAnnIds()\n",
        "    self.img_ids = self.coco.getImgIds()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ann_ids)\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    ann_id = self.ann_ids[idx]\n",
        "    ann = self.coco.loadAnns([ann_id])\n",
        "    img_id = ann[0]['image_id']\n",
        "    img = self.coco.loadImgs([img_id])\n",
        "    \n",
        "    fileN = img[0]['file_name'].split('/')\n",
        "\n",
        "    classN = fileN[0]\n",
        "    \n",
        "    name = classN.split('.')[0]\n",
        "\n",
        "    feature_path = self.dataset + '/features/' + name + '_feature.pt'\n",
        "\n",
        "    features = torch.load(feature_path, map_location=torch.device('cpu'))\n",
        "\n",
        "    ground_truth = ann[0]['category_id']\n",
        "\n",
        "\n",
        "    return ground_truth, features, ann[0]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DBR1sDTz0nIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Train_Dataset(Dataset):\n",
        "  def __init__(self, annotations_file, dataset_dir):  #taxonomy must be: kingdom phylum class order family genus name\n",
        "    self.annotations_file = annotations_file\n",
        "    self.coco = COCO(annotations_file)  #for fast reading\n",
        "    self.dataset = dataset_dir\n",
        "    self.img_ids = self.coco.getImgIds()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_id = self.img_ids[idx]\n",
        "    img = self.coco.loadImgs([img_id])\n",
        "    ann_ids = self.coco.getAnnIds([img_id])\n",
        "    anns = self.coco.loadAnns(ann_ids)\n",
        "    fileN = img[0]['file_name'].split('/')\n",
        "    classN = fileN[0]\n",
        "    direcN = fileN[1]\n",
        "    imgN = fileN[2].split('.')[0]\n",
        "    feature_path = self.dataset + '/features/' + classN + '/' + direcN + '/' + imgN + '_feature.pt'\n",
        "    features = torch.load(feature_path, map_location=torch.device('cpu'))\n",
        "    ground_truth = 0\n",
        "    if len(anns) > 0:\n",
        "      ground_truth = anns[0]['category_id']\n",
        "    return ground_truth, features"
      ],
      "metadata": {
        "id": "ApdVkNgR0qwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgmHg8XqVoPV"
      },
      "source": [
        "\n",
        "dataset_path = '/content/dataset'\n",
        "\n",
        "trainset = Seg_Dataset('TRAIN_ANNOTATIONS.json', dataset_path, 'order') #class order family genus name\n",
        "testset = Seg_Dataset('TRAIN_ANNOTATIONS.json', dataset_path, 'order') \n",
        "\"\"\"\n",
        "dataset_path = '/content/dataset'\n",
        "\n",
        "test_path = 'PATH_TO_DATASET'\n",
        "\n",
        "trainset = Train_Dataset('TRAIN_ANNOTATIONS.json', dataset_path)\n",
        "testset = Test_Dataset('TEST_ANNOTATIONS.json', test_path)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDno7KwGVx98"
      },
      "source": [
        "import random\n",
        "random.seed(7)\n",
        "\n",
        "train_ids = np.arange(0,trainset.__len__(),1)\n",
        "test_ids = np.arange(0,testset.__len__(),1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHazNry9W-N4"
      },
      "source": [
        "X = []\n",
        "Y = []\n",
        "#laden der featurevektoren und groundtruth\n",
        "\n",
        "for i in train_ids:\n",
        "  gt, features = trainset.__getitem__(i)\n",
        "\n",
        "  features = features.reshape(-1)\n",
        "\n",
        "  npfeatures = np.asarray(features)\n",
        "  \n",
        "  X.append(npfeatures)\n",
        "  Y.append(gt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsvqKTQK3EIs"
      },
      "source": [
        "X_arr = np.asarray(X)\n",
        "Y_arr = np.asarray(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_HSqVQ54bAT"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier as kNN\n",
        "\n",
        "clf = kNN(n_neighbors=20)\n",
        "\n",
        "X_svm = X_arr\n",
        "Y_svm = Y_arr\n",
        "\n",
        "clf.fit(X_svm, Y_svm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uWz4nMl3rTq"
      },
      "source": [
        "#from joblib import dump, load\n",
        "\n",
        "#dump(clf, 'PATH.joblib')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing**"
      ],
      "metadata": {
        "id": "9gsxrxBHJN5z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqLHMWbC5fV7"
      },
      "source": [
        "from joblib import dump, load\n",
        "clf = load('MODEL.joblib') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcZ1b9VgejeV"
      },
      "source": [
        "from time import time\n",
        "\n",
        "\n",
        "Ygt = []\n",
        "Ypred = []\n",
        "\n",
        "counter = 1\n",
        "\n",
        "start_time = time() #zeit weil es interessant ist\n",
        "\n",
        "for i in test_ids:\n",
        "  gt, features = testset.__getitem__(i)\n",
        "  features = features.reshape(1, -1)\n",
        "  npfeatures = np.asarray(features)\n",
        "  Ygt.append(gt)\n",
        "  pred = clf.predict(npfeatures)\n",
        "  print(counter)\n",
        "  counter = counter + 1\n",
        "  Ypred.append(pred[0])\n",
        "\n",
        "end_time = time()\n",
        "\n",
        "elapsed = end_time - start_time\n",
        "print(elapsed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbyUTzD6nnst"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix as confusion\n",
        "\n",
        "matr = confusion(Ygt, Ypred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLDWg9l-r7UF"
      },
      "source": [
        "norm_matrix = np.zeros(matr.shape)\n",
        "\n",
        "for i in range(matr.shape[0]):\n",
        "  for j in range(matr.shape[1]):\n",
        "    norm_matrix[i,j] = matr[i,j]/sum(matr[i,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, average_precision_score, f1_score, precision_score, recall_score\n",
        "\n",
        "#metriken\n",
        "accuracy = accuracy_score(Ygt, Ypred)\n",
        "f1 = f1_score(Ygt, Ypred, average = 'macro')\n",
        "precision = precision_score(Ygt, Ypred, average = 'macro')\n",
        "recall = recall_score(Ygt, Ypred, average = 'macro')"
      ],
      "metadata": {
        "id": "77FZOH5iBLte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MzdObuvtKZ1"
      },
      "source": [
        "print(accuracy)\n",
        "print(f1)\n",
        "print(precision)\n",
        "print(recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA für iNat19"
      ],
      "metadata": {
        "id": "Y5XT4xou2EO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "\n",
        "x = X_arr\n",
        "\n",
        "x = StandardScaler().fit_transform(x)\n",
        "y = Y_arr\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "principalComponents = pca.fit_transform(x)\n",
        "\n",
        "principalDf = pd.DataFrame(data = principalComponents\n",
        "             , columns = ['component 1', 'component 2'])\n",
        "\n",
        "\n",
        "targetDf = pd.DataFrame(data = y, columns  = ['target'])\n",
        "\n",
        "finalDf = pd.concat([principalDf, targetDf[['target']]], axis = 1)"
      ],
      "metadata": {
        "id": "-ZPFr_GJ2Fzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "ax = fig.add_subplot(1,1,1) \n",
        "ax.set_xlabel('Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "\n",
        "print(matr.shape)\n",
        "\n",
        "targets = list(np.arange(0, matr.shape[0]+1, 1))\n",
        "\n",
        "def get_cmap(n, name='hsv'):\n",
        "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
        "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
        "    return plt.cm.get_cmap(name, n)\n",
        "\n",
        "new_cmap = get_cmap(matr.shape[0]+1))\n",
        "\n",
        "\n",
        "for i in range(matr.shape[0]+1):\n",
        "    target = targets[i]\n",
        "\n",
        "    indicesToKeep = finalDf['target'] == target\n",
        "    ax.scatter(finalDf.loc[indicesToKeep, 'component 1']\n",
        "               , finalDf.loc[indicesToKeep, 'component 2']\n",
        "               , c = new_cmap(i)\n",
        "               , s = 0.5)\n",
        "ax.grid()"
      ],
      "metadata": {
        "id": "exdL8Q2u3tiy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}