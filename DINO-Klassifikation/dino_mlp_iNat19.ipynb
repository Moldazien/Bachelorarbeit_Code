{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**notebook für die definition von MLPs zur klassifikation mit merkmalsvektoren von dino (Merkmalsvekoren sind schon berechnet) + training von diesen + testen/metriken (für iNat19 Datensatz)**"
      ],
      "metadata": {
        "id": "79mCUnIML0Fg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgL915d4uAab"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HUz2IqPtfLz"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/cocodataset/cocoapi\n",
        "\n",
        "!cd /content/cocoapi/PythonAPI && make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbTFt2YihL9B"
      },
      "outputs": [],
      "source": [
        " !pip install timm\n",
        " \n",
        " !git clone https://github.com/Moldazien/BA.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyfQxEwgMg8R"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/BA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFWM2uT369E_"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/dataset #kopieren von daten aus google drive, weil es dann viel schneller im training geht\n",
        "!cd dataset && mkdir features\n",
        "!cp -R FEATURE_DIRECTORY /content/dataset/features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4No_m7wkJtI2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import cv2\n",
        "import random\n",
        "import colorsys\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "import skimage.io\n",
        "from skimage.measure import find_contours\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Polygon\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms as pth_transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import utils\n",
        "import vision_transformer as vits\n",
        "\n",
        "from pycocotools.coco import COCO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mXt9tcRQbyP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Seg_Dataset(Dataset):\n",
        "  def __init__(self, annotations_file, dataset_dir, taxonomy):  #taxonomy must be: order family genus name (name ist für speziesebene)\n",
        "    self.annotations_file = annotations_file\n",
        "    self.coco = COCO(annotations_file)  \n",
        "    self.dataset = dataset_dir\n",
        "    self.img_ids = self.coco.getImgIds()\n",
        "\n",
        "    def mapping(taxonomy):\n",
        "      mapping = {}\n",
        "      cat_ids = self.coco.getCatIds()\n",
        "      categories = self.coco.loadCats(cat_ids)\n",
        "      by_tax_cat = list(set([cat[taxonomy] for cat in categories]))\n",
        "      by_tax_cat.sort()\n",
        "      numb_cats = len(by_tax_cat)\n",
        "      for i in range(numb_cats):\n",
        "        for cat in categories:\n",
        "          if cat[taxonomy] == by_tax_cat[i]:\n",
        "            mapping[cat['id']] = i + 1\n",
        "      return mapping\n",
        "      \n",
        "    self.cat_mapping = mapping(taxonomy)\n",
        "    self.cat_len = len(self.cat_mapping)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_id = self.img_ids[idx]\n",
        "    img = self.coco.loadImgs(img_id)\n",
        "    ann_ids = self.coco.getAnnIds(img[0]['id'])\n",
        "    anns = self.coco.loadAnns(ann_ids)\n",
        "    fileN = img[0]['file_name'].split('/')\n",
        "    classN = fileN[1]\n",
        "    direcN = fileN[2]\n",
        "    imgN = fileN[3].split('.')[0]\n",
        "    feature_path = self.dataset + '/features/' + classN + '/' + direcN + '/' + imgN + '_feature.pt'\n",
        "    features = torch.load(feature_path, map_location=torch.device('cpu'))\n",
        "    ground_truth = 0\n",
        "    if len(anns) > 0:\n",
        "      ground_truth = self.cat_mapping[anns[0]['category_id']]\n",
        "    gt_vect = np.zeros(self.cat_len)\n",
        "    gt_vect[ground_truth-1] = 1\n",
        "    gt_vect = torch.tensor(gt_vect)\n",
        "\n",
        "    return features, gt_vect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgmHg8XqVoPV"
      },
      "outputs": [],
      "source": [
        "dataset_path = '/content/dataset'\n",
        "\n",
        "taxonomy = 'name' #order family genus name\n",
        "\n",
        "trainset = Seg_Dataset('PATH_TO_TRAIN_ANNOTATION', dataset_path, taxonomy) \n",
        "testset = Seg_Dataset('PATH_TO_TEST_ANNOTATION', dataset_path, taxonomy) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDno7KwGVx98"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(7)\n",
        "train_ids = np.arange(0,trainset.__len__(),1)\n",
        "test_ids = np.arange(0,testset.__len__(),1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ger5houTkWD0"
      },
      "source": [
        "Netzwerke. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9w7-X1fkVYA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MLP_net(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "      super(MLP_net, self).__init__()\n",
        "\n",
        "      self.lin1 = nn.Linear(input_size, input_size, bias=True)\n",
        "      self.lin2 = nn.Linear(input_size, input_size, bias=True)\n",
        "      self.lin2plus = nn.Linear(input_size, input_size, bias=True)\n",
        "      #self.lin2plusplus = nn.Linear(input_size, input_size, bias=True)\n",
        "      self.lin3 = nn.Linear(input_size, int(input_size/2), bias=True)\n",
        "      #self.lin3plus = nn.Linear(int(input_size/2), int(input_size/2), bias=True)\n",
        "      #self.lin3plus1 = nn.Linear(int(input_size/2), int(input_size/2), bias=True)\n",
        "      self.lin4 = nn.Linear(int(input_size/2), output_size, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.lin1(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.lin2(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.lin2plus(x)\n",
        "      x = F.relu(x)\n",
        "      #x = self.lin2plusplus(x)\n",
        "      #x = F.relu(x)\n",
        "      x = self.lin3(x)\n",
        "      x = F.relu(x)\n",
        "      #x = self.lin3plus(x)\n",
        "      #x = F.relu(x)\n",
        "      #x = self.lin3plus1(x)\n",
        "      #x = F.relu(x)\n",
        "      x = self.lin4(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4nw8LLbk-c9"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "dataset_path = '/content/dataset'\n",
        "\n",
        "trainset = Seg_Dataset('PATH_TO_TRAINSET', dataset_path, taxonomy) #class order family genus name\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainset.cat_mapping)\n",
        "print(trainset.cat_len) "
      ],
      "metadata": {
        "id": "b4-I56gZRVtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoGHBwPllAeT"
      },
      "outputs": [],
      "source": [
        "net = MLP_net(768, trainset.cat_len)\n",
        "\n",
        "net.to(device)\n",
        "\n",
        "#PATH = 'MODEL_PATH.pth'\n",
        "#net.load_state_dict(torch.load(PATH))\n",
        "net.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCAf-MNQlB4T"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr = 0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0BH2jSklEDu"
      },
      "outputs": [],
      "source": [
        "#loss_graph = []\n",
        "\n",
        "for epoch in range(30):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, mask = data\n",
        "        inputs = inputs.to(device)\n",
        "        numb_inst = inputs.shape[0] #number of actual batches. falls am ende des datensatzes nicht mehr die volle batchsize vorhanden ist\n",
        "        mask = mask.to(device)\n",
        "        mask = mask.type(torch.float32)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        outputs = outputs.reshape((numb_inst ,175))# 175 ist anzahl an klassen auf speziesebene\n",
        "        loss = criterion(outputs, mask)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        output_value = 500\n",
        "\n",
        "        if i % output_value == output_value - 1: \n",
        "          print('[%d, %5d] loss: %.20f' %\n",
        "              (epoch + 1, i + 1, running_loss / output_value))\n",
        "          loss_graph.append(running_loss / output_value)\n",
        "          running_loss = 0.0\n",
        "          PATH = '/content/gdrive/MyDrive/models/nets/simple/' + str(i) + 'model_mlp.pth'\n",
        "          #torch.save(net.state_dict(), PATH) #checkpoints entfernt, weil das nur viel langsamer wird\n",
        "\n",
        "print('Finished Training')\n",
        "PATH = 'MODEL_SAVE.pth'\n",
        "torch.save(net.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFkATBrK1-H3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_arr = np.asarray(loss_graph)\n",
        "x_val = np.arange(0, len(loss_graph), 1)\n",
        "\n",
        "plt.plot(x_val, loss_arr)\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n_Rt8FI8Nu1"
      },
      "source": [
        "# ab hier testing von anderem stuff\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ3DfdUCG5eG"
      },
      "outputs": [],
      "source": [
        "drive.flush_and_unmount()\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net.eval()\n",
        "net.to(device)"
      ],
      "metadata": {
        "id": "8p8XHv2OAGiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcZ1b9VgejeV"
      },
      "outputs": [],
      "source": [
        "test_ids = np.arange(0,testset.__len__(),1)\n",
        "\n",
        "Ygt = []\n",
        "Ypred = []\n",
        "\n",
        "net.eval()\n",
        "\n",
        "import time\n",
        "start = time.time() #zeit nehmen weil interessant\n",
        "\n",
        "for i in test_ids:\n",
        "  features, gt = testset.__getitem__(i)\n",
        "\n",
        "  features = features.reshape(768)\n",
        "\n",
        "  Ygt.append(np.asarray(gt, dtype=np.float32))\n",
        "  features = features.to(device)\n",
        "  pred = net(features)\n",
        "\n",
        "  pred = F.softmax(pred, dim = 0)\n",
        "\n",
        "  pred = pred.detach()\n",
        "  pred = pred.to('cpu')\n",
        "\n",
        "  Ypred.append(np.asarray(pred, dtype=np.float32))\n",
        "\n",
        "end = time.time()\n",
        "print((end-start)/423)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDhw5t_EXwE5"
      },
      "outputs": [],
      "source": [
        "YclassesGt = []\n",
        "YclassesPred = []\n",
        "\n",
        "#umrechnen der ausgabevektoren der länge 175 aud die tatsächliche klasse (höchsten wert im vektor finden)\n",
        "for elem in Ygt:\n",
        "  index = np.where(elem == 1)\n",
        "  YclassesGt.append(int(index[0]))\n",
        "\n",
        "for elem in Ypred:\n",
        "  top = np.amax(elem)\n",
        "  index = np.where(elem == top)\n",
        "  YclassesPred.append(int(index[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z62xeAiwG7_s"
      },
      "outputs": [],
      "source": [
        "Ypred = YclassesPred  #geht sonst kaputt\n",
        "Ygt = YclassesGt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbyUTzD6nnst"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix as confusion\n",
        "\n",
        "matr = confusion(Ygt, Ypred)  #confusionmatrix berechnen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLDWg9l-r7UF"
      },
      "outputs": [],
      "source": [
        "norm_matrix = np.zeros(matr.shape)\n",
        "#normalisieren\n",
        "for i in range(matr.shape[0]):\n",
        "  for j in range(matr.shape[1]):\n",
        "    norm_matrix[i,j] = matr[i,j]/sum(matr[i,:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jUwukOsq3Ct"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "#ausgabe der confusionmatrizen in schön\n",
        "disp = ConfusionMatrixDisplay(matr)\n",
        "disp.plot()\n",
        "disp1 = ConfusionMatrixDisplay(norm_matrix)\n",
        "disp1.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77FZOH5iBLte"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, average_precision_score, f1_score, precision_score, recall_score\n",
        "\n",
        "#berechnen der verschiedenen metriken\n",
        "accuracy = accuracy_score(Ygt, Ypred)\n",
        "f1 = f1_score(Ygt, Ypred, average = 'macro')\n",
        "precision = precision_score(Ygt, Ypred, average = 'macro')\n",
        "recall = recall_score(Ygt, Ypred, average = 'macro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MzdObuvtKZ1"
      },
      "outputs": [],
      "source": [
        "print(accuracy)\n",
        "print(f1)\n",
        "print(precision)\n",
        "print(recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5XT4xou2EO7"
      },
      "source": [
        "# PCA"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "dino_mlp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
