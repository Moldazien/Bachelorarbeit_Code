{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**notebook um für gegebene bounding boxes mit dino merkmalsvektoren zu berechnen und zu speichern, damit dies nicht immer im training direkt passieren muss**"
      ],
      "metadata": {
        "id": "mPQGRMKWAxa1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgL915d4uAab"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HUz2IqPtfLz"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/cocodataset/cocoapi\n",
        "\n",
        "!cd /content/cocoapi/PythonAPI && make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbTFt2YihL9B"
      },
      "outputs": [],
      "source": [
        " !pip install timm\n",
        " \n",
        " !git clone https://github.com/Moldazien/BA.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyfQxEwgMg8R"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/BA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPyKswMtt-XV"
      },
      "outputs": [],
      "source": [
        "dataset_path  = 'PATH_TO_INAT19'\n",
        "annotations = '/annotations/meg_train.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4No_m7wkJtI2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import cv2\n",
        "import random\n",
        "import colorsys\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "import skimage.io\n",
        "from skimage.measure import find_contours\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Polygon\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms as pth_transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import utils\n",
        "import vision_transformer as vits\n",
        "\n",
        "from pycocotools.coco import COCO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbR6SMssJ3Yw"
      },
      "outputs": [],
      "source": [
        "arch = 'vit_base' #'vit_small''vit_base'\n",
        "patch_size = 8\n",
        "\n",
        "output_dir = '/content/gdrive/MyDrive/Testing/first_test_dino'\n",
        "    \n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "# build model\n",
        "model = vits.__dict__[arch](patch_size=patch_size, num_classes=0)\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "#laden der pretrained weithts für passende modelle   \n",
        "print(\"Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\")\n",
        "url = None\n",
        "if arch == \"vit_small\" and patch_size == 16:\n",
        "    url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n",
        "elif arch == \"vit_small\" and patch_size == 8:\n",
        "    url = \"dino_deitsmall8_300ep_pretrain/dino_deitsmall8_300ep_pretrain.pth\"  # model used for visualizations in our paper\n",
        "elif arch == \"vit_base\" and patch_size == 16:\n",
        "    url = \"dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\"\n",
        "elif arch == \"vit_base\" and patch_size == 8:\n",
        "    url = \"dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\"\n",
        "if url is not None:\n",
        "    print(\"Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\")\n",
        "    state_dict = torch.hub.load_state_dict_from_url(url=\"https://dl.fbaipublicfiles.com/dino/\" + url)\n",
        "    model.load_state_dict(state_dict, strict=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-aEiKdpeBZ2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "#funktion um bildausschnitte mit rand zu croppen\n",
        "def crop_fkt(bbox, img, area, percentile):\n",
        "  width, height = img.size\n",
        "\n",
        "  width_edge = bbox[2]*percentile\n",
        "  height_edge = bbox[3]*percentile\n",
        "\n",
        "  edge = (width_edge + height_edge)\n",
        "\n",
        "  tx = max(bbox[0]-edge, 0)\n",
        "  ty = max(bbox[1]-edge, 0)\n",
        "  bx = min(bbox[0]+bbox[2]+edge, width)\n",
        "  by = min(bbox[1]+bbox[3]+edge, height)\n",
        "\n",
        "  h_box = bx - tx\n",
        "  w_box = by - ty\n",
        "\n",
        "  scaling_factor = np.sqrt(area/(h_box*w_box))\n",
        "\n",
        "  h_scale = int(np.round(h_box*scaling_factor))\n",
        "  w_scale = int(np.round(w_box*scaling_factor)) \n",
        "\n",
        "  return (tx, ty, bx, by), (w_scale, h_scale), edge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNd9Jx3LNTxP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "#funktion um bilder direkt mit dino zu merkmalsvektoren zu verarbeiten\n",
        "\n",
        "def DINO_fwd(model, image, bbox):\n",
        "  percentile = 0.3 \n",
        "  patch_size = 8\n",
        "  image_size = (480, 480)\n",
        "  area = 230000 \n",
        "\n",
        "  box, image_size, edge = crop_fkt(bbox, image, area, percentile)\n",
        "\n",
        "  crop_img = image.crop(box=box)\n",
        "\n",
        "  transform = pth_transforms.Compose([\n",
        "                                      pth_transforms.Resize(200, interpolation=3),\n",
        "                                      pth_transforms.ToTensor(),\n",
        "                                      pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "                                      ])\n",
        "  crop_img = transform(crop_img)\n",
        "\n",
        "  w, h = crop_img.shape[1] - crop_img.shape[1] % patch_size, crop_img.shape[2] - crop_img.shape[2] % patch_size\n",
        "  crop_img = crop_img[:, :w, :h].unsqueeze(0)\n",
        "\n",
        "  features = model(crop_img.to(device)).clone()\n",
        "  features = features.detach().to(torch.device('cpu'))\n",
        "\n",
        "  return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVZpKj3i_j4B"
      },
      "source": [
        "datenklasse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp1-15qH_jUY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class NACTI_seg(Dataset):\n",
        "  def __init__(self, annotations_file, dataset_dir):\n",
        "    self.annotations_file = annotations_file\n",
        "    self.coco = COCO(annotations_file)  #for fast reading\n",
        "    \n",
        "    \n",
        "    self.dataset = dataset_dir\n",
        "    self.img_ids = self.coco.getImgIds()\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_ids)\n",
        "\n",
        "  #nicht ganz korrekt, gibt RGB bild zurück statt direkten tensor\n",
        "  def __getitem__(self, idx):\n",
        "    img_id = self.img_ids[idx]\n",
        "\n",
        "    img = self.coco.loadImgs(img_id)\n",
        "\n",
        "    ann_ids = self.coco.getAnnIds(img[0]['id'])\n",
        "    anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "    img_path = self.dataset + '/images/' + img[0]['file_name']\n",
        "    image = None\n",
        "    mask = None\n",
        "    bbox = None\n",
        "    with open(img_path, 'rb') as f:\n",
        "        image = Image.open(f)\n",
        "        image = image.convert('RGB')\n",
        "    \n",
        "    if len(anns) > 0:\n",
        "      bbox = anns[0]['bbox']\n",
        "    else:\n",
        "      bbox = [0,0,img[0]['width'], img[0]['height']]\n",
        "\n",
        "    features = DINO_fwd(model, image, bbox)\n",
        "\n",
        "\n",
        "    return features, img[0]['id'], img[0]['file_name']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRDCYbYh1-Ho"
      },
      "outputs": [],
      "source": [
        "%time #zeit messen, weils interessant ist\n",
        "\n",
        "starting_point = 0\n",
        "\n",
        "import os\n",
        "\n",
        "dataset_path = 'PATH_TO_iNaturalist19'\n",
        "feature_direc = '/features/'\n",
        "\n",
        "\n",
        "trainset = NACTI_seg('PATH_TO_iNaturalist19_ANNOTATION', dataset_path)\n",
        "\n",
        "for j in range(trainset.__len__() - starting_point):\n",
        "\n",
        "  i = j + starting_point\n",
        "\n",
        "  features, img_id, file_name = trainset.__getitem__(i)\n",
        "\n",
        "  fileN = file_name.split('/')\n",
        "\n",
        "  classN = fileN[1]\n",
        "  direcN = fileN[2]\n",
        "  imgN = fileN[3].split('.')[0]\n",
        "\n",
        "  if not os.path.exists(dataset_path + feature_direc + '/' + classN):\n",
        "    os.mkdir(dataset_path + feature_direc + '/' + classN)\n",
        "  \n",
        "  if not os.path.exists(dataset_path + feature_direc + '/' + classN + '/' + direcN):\n",
        "    os.mkdir(dataset_path + feature_direc + '/' + classN + '/' + direcN)\n",
        "\n",
        "  if i % 500 == 0:\n",
        "    drive.flush_and_unmount()\n",
        "\n",
        "    drive.mount('/content/gdrive')\n",
        "    print(str(i) + ' von ' + str(trainset.__len__()) + '     ' + str(features.shape))\n",
        "\n",
        "  feature_path = dataset_path + feature_direc + '/' + classN + '/' + direcN + '/' + imgN + '_feature.pt'\n",
        "  \n",
        "  torch.save(features.clone(), feature_path)\n",
        "  torch.save(mask, mask_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQpIjB6rKho"
      },
      "source": [
        "# saving in gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0Zys3AxGPep",
        "outputId": "44e299a8-58b5-48fc-858d-b2686059be02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "dataset_to_featuretensor_iNat.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}