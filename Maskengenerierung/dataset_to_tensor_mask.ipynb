{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset_to_tensor_mask.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**notebook um für bildausschnitte aus bilderen eines datensatzes mit dino attentionmaps zwischen zu speichern, damit das nicht immer direkt im training geschehen muss**"
      ],
      "metadata": {
        "id": "4VsGsnyCCnlV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgL915d4uAab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HUz2IqPtfLz"
      },
      "source": [
        "!git clone https://github.com/cocodataset/cocoapi\n",
        "\n",
        "!cd /content/cocoapi/PythonAPI && make"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbTFt2YihL9B"
      },
      "source": [
        " !pip install timm\n",
        " \n",
        " !git clone https://github.com/Moldazien/BA.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyfQxEwgMg8R"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/BA')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4No_m7wkJtI2"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import cv2\n",
        "import random\n",
        "import colorsys\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "import skimage.io\n",
        "from skimage.measure import find_contours\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Polygon\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms as pth_transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import utils\n",
        "import vision_transformer as vits\n",
        "\n",
        "from pycocotools.coco import COCO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbR6SMssJ3Yw"
      },
      "source": [
        "arch = 'vit_base' #'vit_small''vit_base'\n",
        "patch_size = 8\n",
        "\n",
        "output_dir = '/content/gdrive/MyDrive/Testing/first_test_dino'\n",
        "    \n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "# build model\n",
        "model = vits.__dict__[arch](patch_size=patch_size, num_classes=0)\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "#laden der pretrained weithts für passende modelle   \n",
        "print(\"Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\")\n",
        "url = None\n",
        "if arch == \"vit_small\" and patch_size == 16:\n",
        "    url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n",
        "elif arch == \"vit_small\" and patch_size == 8:\n",
        "    url = \"dino_deitsmall8_300ep_pretrain/dino_deitsmall8_300ep_pretrain.pth\"  # model used for visualizations in our paper\n",
        "elif arch == \"vit_base\" and patch_size == 16:\n",
        "    url = \"dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\"\n",
        "elif arch == \"vit_base\" and patch_size == 8:\n",
        "    url = \"dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\"\n",
        "if url is not None:\n",
        "    print(\"Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\")\n",
        "    state_dict = torch.hub.load_state_dict_from_url(url=\"https://dl.fbaipublicfiles.com/dino/\" + url)\n",
        "    model.load_state_dict(state_dict, strict=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-aEiKdpeBZ2"
      },
      "source": [
        "import numpy as np\n",
        "#komplexe cropfunktion, um später die attentionmaps auf originale bounding boxe zu zuschneiden\n",
        "def crop_fkt(bbox, img, area, percentile):\n",
        "  width, height = img.size\n",
        "\n",
        "  width_edge = bbox[2]*percentile\n",
        "  height_edge = bbox[3]*percentile\n",
        "\n",
        "  edge = int(width_edge + height_edge)\n",
        "\n",
        "  tx = max(bbox[0]-edge, 0)\n",
        "  ty = max(bbox[1]-edge, 0)\n",
        "  bx = min(bbox[0]+bbox[2]+edge, width)\n",
        "  by = min(bbox[1]+bbox[3]+edge, height)\n",
        "\n",
        "  #find out where edge was added\n",
        "  left = bbox[0] - tx \n",
        "  up = bbox[1] - ty\n",
        "  right = bx - (bbox[0]+bbox[2])\n",
        "  down = by - (bbox[1]+bbox[3])\n",
        "\n",
        "  added_edge = [left, up, right, down]\n",
        "\n",
        "  area = 280000\n",
        "  wid = bx - tx\n",
        "  hei = by - ty\n",
        "  #print((wid, hei))\n",
        "\n",
        "  p = np.sqrt(area/(wid*hei))\n",
        "\n",
        "  imagewidth = int(wid*p) - (int(wid*p)%8)\n",
        "  imageheight = int(hei*p) - (int(hei*p)%8)\n",
        "\n",
        "  img_size = (imageheight, imagewidth)\n",
        "\n",
        "  return (tx, ty, bx, by), added_edge, p, img_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNd9Jx3LNTxP"
      },
      "source": [
        "import numpy as np\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "def DINO_fwd(model, image, maskimg, bbox):\n",
        "  percentile = 0.3\n",
        "  patch_size = 8 \n",
        "  area = 230000 \n",
        "\n",
        "  box, added_edge, p, image_size = crop_fkt(bbox, image, area, percentile)\n",
        "\n",
        "  crop_img = image.crop(box=box)\n",
        "  transform = pth_transforms.Compose([\n",
        "                                    pth_transforms.Resize(image_size),\n",
        "                                    pth_transforms.ToTensor(),\n",
        "                                    pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "                                    ])\n",
        "  crop_img = transform(crop_img)\n",
        "\n",
        "  w, h = crop_img.shape[1] - crop_img.shape[1] % patch_size, crop_img.shape[2] - crop_img.shape[2] % patch_size\n",
        "  crop_img = crop_img[:, :w, :h].unsqueeze(0)\n",
        "  w_featmap = crop_img.shape[-2] // patch_size\n",
        "  h_featmap = crop_img.shape[-1] // patch_size\n",
        "\n",
        "  attentions = model.get_last_selfattention(crop_img.to(device))\n",
        "\n",
        "  nh = attentions.shape[1]\n",
        "\n",
        "  attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
        "  attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
        "  attentions_transform = pth_transforms.Compose([\n",
        "                                                  pth_transforms.Resize(image_size, interpolation = InterpolationMode('nearest'))\n",
        "                                                  ])\n",
        "  attentions = attentions_transform(attentions)\n",
        "\n",
        "\n",
        "\n",
        "  attentions = attentions[:, int(added_edge[1]*p): int((added_edge[1] + bbox[3])*p), int(added_edge[0]*p):int((added_edge[0] + bbox[2])*p)]\n",
        "\n",
        "  crop_mask = maskimg.crop(box=[bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]])\n",
        "\n",
        "  mask = np.asarray(crop_mask)\n",
        "  mask = torch.tensor(mask)\n",
        "\n",
        "  return attentions, mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVZpKj3i_j4B"
      },
      "source": [
        "custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp1-15qH_jUY"
      },
      "source": [
        "import os\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class NACTI_seg(Dataset):\n",
        "  def __init__(self, annotations_file, dataset_dir):\n",
        "    self.annotations_file = annotations_file\n",
        "    self.coco = COCO(annotations_file)  #for fast reading\n",
        "    \n",
        "    self.dataset = dataset_dir\n",
        "    self.img_ids = self.coco.getImgIds()\n",
        "    self.ann_ids = self.coco.getAnnIds()\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ann_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    ann_id = self.ann_ids[idx]\n",
        "    ann = self.coco.loadAnns([ann_id])\n",
        "    img_id = ann[0]['image_id']\n",
        "    img = self.coco.loadImgs([img_id])\n",
        "\n",
        "\n",
        "    img_path = self.dataset + '/images/' + img[0]['file_name']\n",
        "    image = None\n",
        "    mask = None\n",
        "    bbox = None\n",
        "    with open(img_path, 'rb') as f:\n",
        "        image = Image.open(f)\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "    mask = self.coco.annToMask(ann[0]) #mask for instance segmentation\n",
        "    #mask = np.zeros((2000, 2000))\n",
        "    \n",
        "    bbox = ann[0]['bbox']\n",
        "    maskImg = Image.fromarray(mask)\n",
        "    attentions, mask = DINO_fwd(model, image, maskImg, bbox)\n",
        "\n",
        "    return attentions, mask,  ann_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRDCYbYh1-Ho"
      },
      "source": [
        "#tats#chliches berechnen der attentionmaps und speichern dieser\n",
        "\n",
        "dataset_path = 'PATH_TO_DATASET'\n",
        "\n",
        "tensor_direc = '/tensors/'\n",
        "\n",
        "\n",
        "trainset = NACTI_seg('PATH_TO_DATASET_ANNOTATIONS.JSON', dataset_path) #meg_\n",
        "\n",
        "for i in range(trainset.__len__()):\n",
        "  attentions, mask, img_id  = trainset.__getitem__(i)\n",
        "\n",
        "  print(str(i) + ' von ' + str(trainset.__len__()) + '     ' + str(attentions.shape))\n",
        "\n",
        "  attentions_path = dataset_path + tensor_direc + str(img_id) + '_attention.pt'\n",
        "\n",
        "  torch.save(attentions.clone(), attentions_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "oFyjAiF4A0rn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e585ddb8-da57-4b61-97f3-ae261c4ae6af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n"
          ]
        }
      ]
    }
  ]
}